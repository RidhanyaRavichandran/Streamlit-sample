# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ZS5S9wiRpnPEHSno2RVOMqT20WaiYfv
"""

# --- 1. Imports ---
import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, GlobalAveragePooling1D

import speech_recognition as sr
import moviepy.editor as mp

!pip install SpeechRecognition

# --- 2. Text Cleaning Function ---
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# --- 3. Audio ‚Üí Text Conversion ---
def audio_to_text(audio_path):
    recognizer = sr.Recognizer()
    try:
        with sr.AudioFile(audio_path) as source:
            audio_data = recognizer.record(source)
            text = recognizer.recognize_google(audio_data)
            return clean_text(text)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not process audio file {audio_path}: {e}")
        return ""

# --- 4. Video ‚Üí Text Conversion ---
def video_to_text(video_path):
    try:
        clip = mp.VideoFileClip(video_path)
        audio_path = "temp_audio.wav"
        clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
        text = audio_to_text(audio_path)
        os.remove(audio_path)
        return text
    except Exception as e:
        print(f"‚ö†Ô∏è Could not process video file {video_path}: {e}")
        return ""

# --- 5. Prepare Unified Dataset (Multi CSV + Audio + Video + Balancing) ---

import os
import pandas as pd
from sklearn.utils import resample

def prepare_unified_dataset(
    text_dirs=None,                # List of text dataset CSV paths
    audio_dir=None,                # Path to audio folder (organized by label)
    video_dir=None,                # Path to video folder (organized by label)
    output_csv="/content/final_balanced_dataset.csv"
):
    """
    Combine text, audio, and video inputs into one balanced dataset CSV.
    Converts audio/video to text before merging.
    """

    data = []

    # --- Load multiple text datasets ---
    if text_dirs:
        for text_path in text_dirs:
            if os.path.exists(text_path):
                print(f"üìò Loading text dataset: {text_path}")
                df_temp = pd.read_csv(text_path)
                if "text" not in df_temp.columns or "label" not in df_temp.columns:
                    print(f"‚ö†Ô∏è Skipped {text_path} (missing 'text' or 'label')")
                    continue
                df_temp["text"] = df_temp["text"].apply(clean_text)
                for _, row in df_temp.iterrows():
                    data.append({"text": row["text"], "label": row["label"]})
            else:
                print(f"‚ùå File not found: {text_path}")

    # --- Convert audio files to text ---
    if audio_dir and os.path.exists(audio_dir):
        print("üéß Converting audio files to text...")
        for label in os.listdir(audio_dir):
            folder = os.path.join(audio_dir, label)
            if not os.path.isdir(folder):
                continue
            for file in os.listdir(folder):
                if not file.lower().endswith((".wav", ".mp3")):
                    continue
                path = os.path.join(folder, file)
                text = audio_to_text(path)
                if text:
                    data.append({"text": clean_text(text), "label": label})

    # --- Convert video files to text ---
    if video_dir and os.path.exists(video_dir):
        print("üé¨ Converting video files to text...")
        for label in os.listdir(video_dir):
            folder = os.path.join(video_dir, label)
            if not os.path.isdir(folder):
                continue
            for file in os.listdir(folder):
                if not file.lower().endswith((".mp4", ".mov", ".avi")):
                    continue
                path = os.path.join(folder, file)
                text = video_to_text(path)
                if text:
                    data.append({"text": clean_text(text), "label": label})

    # --- Combine and Clean ---
    df = pd.DataFrame(data)
    df.drop_duplicates(subset="text", inplace=True)
    df.reset_index(drop=True, inplace=True)
    print(f"‚úÖ Combined dataset size before balancing: {len(df)}")

    # --- Balance Dataset ---
    if not df.empty and "label" in df.columns:
        min_samples = df["label"].value_counts().min()
        balanced_df = (
            df.groupby("label", group_keys=False)
              .apply(lambda x: resample(x, replace=False, n_samples=min_samples, random_state=42))
              .reset_index(drop=True)
        )
        print(f"‚úÖ Dataset balanced successfully! Each class now has {min_samples} samples.")
    else:
        balanced_df = df
        print("‚ö†Ô∏è Skipping balancing (no label column found).")

    # --- Save to CSV ---
    balanced_df.to_csv(output_csv, index=False)
    print(f"üíæ Unified balanced dataset saved to {output_csv} ({len(balanced_df)} samples)")

    return balanced_df

# --- 6. Load Unified Dataset (Single CSV) ---

import pandas as pd

# Specify the path to the single dataset
dataset_path = "/content/cb_multi_labeled_balanced.csv"

print(f"üìÇ Loading dataset: {dataset_path}")
try:
    df = pd.read_csv(dataset_path, encoding='utf-8')
except UnicodeDecodeError:
    print(f"Trying to read {dataset_path} with latin-1 encoding...")
    df = pd.read_csv(dataset_path, encoding='latin-1')
except FileNotFoundError:
    print(f"‚ùå Error: File not found at {dataset_path}")
    df = pd.DataFrame() # Create an empty DataFrame if file not found
except Exception as e:
    print(f"‚ùå Error reading {dataset_path}: {e}")
    df = pd.DataFrame() # Create an empty DataFrame on other errors


if not df.empty:
    # Ensure essential columns exist
    if "text" not in df.columns or "label" not in df.columns:
        print(f"‚ö†Ô∏è Skipping cleaning: Missing 'text' or 'label' column in {dataset_path}")
    else:
        # Clean text
        df["text"] = df["text"].apply(clean_text)
        df = df[["text", "label"]] # Keep only essential columns


    print(f"‚úÖ Dataset size: {len(df)}")
    print(df.head())
else:
    print("‚ùå No valid dataset was loaded.")

# --- 7. Encode Labels ---
label_encoder = LabelEncoder()
y = to_categorical(label_encoder.fit_transform(df["label"]))
print("\nLabel mapping:", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))

# Save the label encoder
import joblib
joblib.dump(label_encoder, "/content/label_encoder.pkl")
print("‚úÖ Label encoder saved as /content/label_encoder.pkl")

# --- 8. Tokenize and Pad Text ---
MAX_WORDS = 10000
MAX_LEN = 100

tokenizer = Tokenizer(num_words=MAX_WORDS)
tokenizer.fit_on_texts(df["text"])
X = pad_sequences(tokenizer.texts_to_sequences(df["text"]), maxlen=MAX_LEN)

# Save the tokenizer
import joblib
joblib.dump(tokenizer, "/content/tokenizer.pkl")
print("‚úÖ Tokenizer saved as /content/tokenizer.pkl")

# --- 9. Train/Test Split ---
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
print(f"\nTraining samples: {X_train.shape[0]}")
print(f"Validation samples: {X_val.shape[0]}")

# --- 10. Build CNN Model ---
def build_text_cnn(num_classes):
    model = Sequential([
        Embedding(MAX_WORDS, 64, input_length=MAX_LEN),   # smaller embedding
        Conv1D(64, 3, activation='relu'),                 # fewer filters, smaller kernel
        GlobalAveragePooling1D(),                         # faster pooling
        Dense(32, activation='relu'),                     # smaller dense layer
        Dropout(0.3),                                     # slightly less dropout
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    return model

# Build the optimized model
model = build_text_cnn(num_classes=y.shape[1])
model.summary()

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=5,           # reduce from 10 ‚Üí 5
    batch_size=64,      # increase batch size for fewer updates per epoch
    verbose=1
)

# --- 12. Evaluate Model ---
loss, acc = model.evaluate(X_val, y_val, verbose=0)
print(f"\n‚úÖ Validation Accuracy: {acc*100:.2f}%")
print(f"‚úÖ Validation Loss: {loss:.4f}")

# --- 13. Accuracy & Loss Visualization ---
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Val Accuracy")
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

# --- 14. Confusion Matrix ---
y_pred = np.argmax(model.predict(X_val), axis=1)
y_true = np.argmax(y_val, axis=1)

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

# --- 15. Save Model ---
model.save("/content/cyberbullying_cnn_model.h5")
print("\n‚úÖ Model saved as 'cyberbullying_cnn_model.h5'")

# --- 16. Real-Time Prediction Function ---
def predict_text(text):
    cleaned = clean_text(text)
    seq = pad_sequences(tokenizer.texts_to_sequences([cleaned]), maxlen=MAX_LEN)
    pred = model.predict(seq)
    label = label_encoder.classes_[np.argmax(pred)]
    confidence = np.max(pred)
    return label, confidence

!pip install streamlit

!pip install deep-translator

!pip install streamlit-lottie

!pip install streamlit transformers torch seaborn matplotlib wordcloud speechrecognition moviepy pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import torch
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# import numpy as np
# import pandas as pd
# import re
# import tempfile
# import os
# import speech_recognition as sr
# import moviepy.editor as mp
# import matplotlib.pyplot as plt
# import seaborn as sns
# from wordcloud import WordCloud
# 
# # --- CONFIG ---
# st.set_page_config(page_title="Cyberbullying Detection Dashboard", page_icon="ü§ñ", layout="wide")
# MODEL_NAME = "unitary/toxic-bert"
# 
# # --- LOAD MODEL ---
# @st.cache_resource
# def load_model():
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
#     model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
#     return tokenizer, model
# 
# tokenizer, model = load_model()
# 
# # --- CLEAN TEXT ---
# def clean_text(text):
#     text = re.sub(r"http\S+|www\S+|https\S+", "", text)
#     text = re.sub(r"@\w+|#\w+", "", text)
#     text = re.sub(r"[^A-Za-z0-9\s!?.,]", "", text)
#     return text.strip()
# 
# # --- AUDIO TO TEXT ---
# def audio_to_text(audio_path):
#     recognizer = sr.Recognizer()
#     try:
#         with sr.AudioFile(audio_path) as source:
#             audio_data = recognizer.record(source)
#             return recognizer.recognize_google(audio_data)
#     except Exception as e:
#         st.error(f"Audio error: {e}")
#         return ""
# 
# # --- VIDEO TO TEXT ---
# def video_to_text(video_path):
#     try:
#         clip = mp.VideoFileClip(video_path)
#         if clip.audio is None:
#             st.error("‚ö†Ô∏è Video has no audio track.")
#             return ""
#         with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
#             clip.audio.write_audiofile(temp_audio.name, verbose=False, logger=None)
#             text = audio_to_text(temp_audio.name)
#         os.remove(temp_audio.name)
#         return text
#     except Exception as e:
#         st.error(f"Video error: {e}")
#         return ""
# 
# # --- PREDICT ---
# def predict_text(text, threshold=0.5):
#     cleaned = clean_text(text)
#     inputs = tokenizer(cleaned, return_tensors="pt", truncation=True, padding=True)
#     with torch.no_grad():
#         outputs = model(**inputs)
#         probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
#     probs_np = probs.detach().cpu().numpy()[0]
#     labels = [model.config.id2label[i] for i in range(len(probs_np))]
# 
#     # --- Determine toxicity level ---
#     toxic_prob = probs_np[1] if "toxic" in labels[1].lower() else probs_np[0]
#     prediction = "Toxic" if toxic_prob >= threshold else "Non-Toxic"
# 
#     return prediction, probs_np, labels, toxic_prob
# 
# # --- VISUALIZATIONS ---
# def show_confidence_bar(labels, probs):
#     st.markdown("### üéØ Model Confidence per Class")
#     fig, ax = plt.subplots(figsize=(6, 3))
#     sns.barplot(x=probs, y=labels, ax=ax, palette="viridis")
#     ax.set_xlabel("Confidence Score")
#     ax.set_ylabel("Class")
#     st.pyplot(fig, clear_figure=True)
# 
# def show_wordcloud(text):
#     if not text.strip():
#         st.warning("‚ö†Ô∏è No text for word cloud.")
#         return
#     wc = WordCloud(width=600, height=300, background_color="white", colormap="cool").generate(text)
#     st.image(wc.to_array(), caption="WordCloud of Input Text", use_container_width=True)
# 
# def show_dashboard(pred_history):
#     st.markdown("## üìä Real-Time Prediction Analytics")
#     if len(pred_history) == 0:
#         st.info("No predictions yet.")
#         return
# 
#     df = pd.DataFrame(pred_history, columns=["Text", "Prediction", "Confidence"])
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.markdown("### Prediction Class Distribution")
#         fig1, ax1 = plt.subplots()
#         sns.countplot(data=df, x="Prediction", palette="pastel", ax=ax1)
#         st.pyplot(fig1, clear_figure=True)
# 
#     with col2:
#         st.markdown("### Confidence Trend Over Time")
#         fig2, ax2 = plt.subplots()
#         sns.lineplot(data=df, x=df.index, y="Confidence", hue="Prediction", marker="o", palette="muted", ax=ax2)
#         ax2.set_ylim(0, 1)
#         st.pyplot(fig2, clear_figure=True)
# 
#     st.dataframe(df.tail(10), use_container_width=True)
# 
# # --- STREAMLIT UI ---
# st.title("ü§ñ Cyberbullying Detection & Visualization Dashboard")
# st.write("Detect toxic and non-toxic content from text, audio, or video inputs ‚Äî with real-time analytics.")
# 
# if "pred_history" not in st.session_state:
#     st.session_state["pred_history"] = []
# 
# option = st.sidebar.radio("Choose Input Type:", ["Text", "Audio", "Video"])
# 
# if option == "Text":
#     st.subheader("üìù Enter Text")
#     text_input = st.text_area("Type or paste text here:")
#     if st.button("üîç Predict"):
#         if text_input.strip():
#             prediction, probs, labels, toxic_prob = predict_text(text_input)
#             if prediction == "Toxic":
#                 st.error(f"üö® Prediction: **{prediction}**")
#             else:
#                 st.success(f"üåø Prediction: **{prediction}**")
#             st.info(f"Toxicity Confidence: {toxic_prob * 100:.2f}%")
#             st.session_state["pred_history"].append([text_input, prediction, toxic_prob])
#             show_confidence_bar(labels, probs)
#             show_wordcloud(text_input)
#         else:
#             st.warning("Please enter text.")
# 
# elif option == "Audio":
#     st.subheader("üéß Upload Audio File")
#     uploaded_audio = st.file_uploader("Upload a .wav file", type=["wav"])
#     if uploaded_audio:
#         with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp:
#             temp.write(uploaded_audio.read())
#             text = audio_to_text(temp.name)
#         if text:
#             st.info(f"üó£ Transcribed: {text}")
#             prediction, probs, labels, toxic_prob = predict_text(text)
#             if prediction == "Toxic":
#                 st.error(f"üö® Prediction: **{prediction}**")
#             else:
#                 st.success(f"üåø Prediction: **{prediction}**")
#             st.session_state["pred_history"].append([text, prediction, toxic_prob])
#             show_confidence_bar(labels, probs)
#             show_wordcloud(text)
# 
# elif option == "Video":
#     st.subheader("üé¨ Upload Video File")
#     uploaded_video = st.file_uploader("Upload a .mp4 file", type=["mp4"])
#     if uploaded_video:
#         with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp:
#             temp.write(uploaded_video.read())
#             text = video_to_text(temp.name)
#         if text:
#             st.info(f"üé• Extracted Speech: {text}")
#             prediction, probs, labels, toxic_prob = predict_text(text)
#             if prediction == "Toxic":
#                 st.error(f"üö® Prediction: **{prediction}**")
#             else:
#                 st.success(f"üåø Prediction: **{prediction}**")
#             st.session_state["pred_history"].append([text, prediction, toxic_prob])
#             show_confidence_bar(labels, probs)
#             show_wordcloud(text)
# 
# st.markdown("---")
# show_dashboard(st.session_state["pred_history"])
# st.caption("Built with ‚ù§Ô∏è using Streamlit + Transformers (ToxicBERT)")
#

!pip install pyngrok

from pyngrok import ngrok

# Replace with your ngrok authtoken
!ngrok config add-authtoken 33gKetPNDOHzjU5ZLInR4lqVVad_7dducXcz4H2gRRRsGuCYB
# Run Streamlit in background
!nohup streamlit run app.py --server.port 8501 --server.headless true &

# Open tunnel
public_url = ngrok.connect(8501)
print("üåç Your Streamlit App URL:", public_url)